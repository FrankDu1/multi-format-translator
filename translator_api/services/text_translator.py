"""
æ–‡æœ¬ç¿»è¯‘æœåŠ¡
ä½¿ç”¨ NLLB æ¨¡å‹è¿›è¡Œçº¯æ–‡æœ¬ç¿»è¯‘
"""

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch
from logger_config import app_logger
import re

# å…¨å±€æ¨¡å‹ç¼“å­˜
_model = None
_tokenizer = None



def load_translation_model():
    """åŠ è½½ç¿»è¯‘æ¨¡å‹ï¼ˆæ‡’åŠ è½½ï¼‰"""
    global _model, _tokenizer
    
    if _model is None or _tokenizer is None:
        try:
            app_logger.info("ğŸ”„ åŠ è½½ NLLB ç¿»è¯‘æ¨¡å‹...")
            
            model_name = "facebook/nllb-200-distilled-600M"
            
            _tokenizer = AutoTokenizer.from_pretrained(model_name)
            _model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
            
            # å¦‚æœæœ‰ GPUï¼Œä½¿ç”¨ GPU
            if torch.cuda.is_available():
                _model = _model.to('cuda')
                app_logger.info("âœ“ æ¨¡å‹å·²åŠ è½½åˆ° GPU")
            else:
                app_logger.info("âœ“ æ¨¡å‹å·²åŠ è½½åˆ° CPU")
            
        except Exception as e:
            app_logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    return _model, _tokenizer


def split_text_into_chunks(text, max_length=400):
    """
    æŒ‰æœ€å¤§é•¿åº¦åˆ†å‰²æ–‡æœ¬ï¼Œé¿å…æ¨¡å‹æˆªæ–­
    
    Args:
        text: åŸå§‹æ–‡æœ¬
        max_length: æ¯å—æœ€å¤§å­—ç¬¦æ•°
    
    Returns:
        list: æ–‡æœ¬å—åˆ—è¡¨
    """
    import re
    sentences = re.split(r'([ã€‚ï¼ï¼Ÿ.!?])', text)
    chunks = []
    chunk = ''
    for i in range(0, len(sentences), 2):
        part = sentences[i]
        sep = sentences[i+1] if i+1 < len(sentences) else ''
        if len(chunk) + len(part) + len(sep) > max_length and chunk:
            chunks.append(chunk)
            chunk = ''
        chunk += part + sep
    if chunk:
        chunks.append(chunk)
    return [c.strip() for c in chunks if c.strip()]


def translate_chunk(text, model, tokenizer, src_lang, tgt_lang):
    """
    ç¿»è¯‘å•ä¸ªæ–‡æœ¬å—
    
    Args:
        text: æ–‡æœ¬å—
        model: ç¿»è¯‘æ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        src_lang: æºè¯­è¨€ä»£ç 
        tgt_lang: ç›®æ ‡è¯­è¨€ä»£ç 
    
    Returns:
        str: ç¿»è¯‘åçš„æ–‡æœ¬
    """
    try:
        # è®¾ç½®æºè¯­è¨€
        tokenizer.src_lang = src_lang
        
        # ç¼–ç 
        inputs = tokenizer(
            text, 
            return_tensors="pt", 
            padding=True, 
            truncation=True, 
            max_length=400
        )
        
        # å¦‚æœæ¨¡å‹åœ¨ GPU ä¸Šï¼Œè¾“å…¥ä¹Ÿè¦åœ¨ GPU ä¸Š
        if torch.cuda.is_available():
            inputs = {k: v.to('cuda') for k, v in inputs.items()}
        
        # è®¾ç½®ç›®æ ‡è¯­è¨€
        model.config.forced_bos_token_id = tokenizer.convert_tokens_to_ids(f"<2{tgt_lang}>")
        
        # ç”Ÿæˆç¿»è¯‘
        translated_tokens = model.generate(
            **inputs,
            max_length=600,
            num_beams=5,
            early_stopping=True
        )
        
        # è§£ç 
        translated_text = tokenizer.batch_decode(
            translated_tokens, 
            skip_special_tokens=True
        )[0]
        
        return translated_text
    
    except Exception as e:
        app_logger.error(f"âŒ æ–‡æœ¬å—ç¿»è¯‘å¤±è´¥: {e}")
        return text  # è¿”å›åŸæ–‡


def translate_text_with_nllb(text, src_lang='en', tgt_lang='zh'):
    """
    ä½¿ç”¨ NLLB æ¨¡å‹ç¿»è¯‘æ–‡æœ¬ï¼ˆæ”¯æŒé•¿æ–‡æœ¬åˆ†æ®µç¿»è¯‘ï¼‰
    
    Args:
        text: è¦ç¿»è¯‘çš„æ–‡æœ¬
        src_lang: æºè¯­è¨€ä»£ç  (en/zh/auto)
        tgt_lang: ç›®æ ‡è¯­è¨€ä»£ç  (en/zh)
    
    Returns:
        str: ç¿»è¯‘åçš„æ–‡æœ¬
    """
    try:
        # åŠ è½½æ¨¡å‹
        model, tokenizer = load_translation_model()
        
        # è½¬æ¢è¯­è¨€ä»£ç , NLLB è¯­è¨€ä»£ç æ˜ å°„
        
        app_logger.info(f"ğŸŒ ç¿»è¯‘: {src_lang} â†’ {tgt_lang}")
        app_logger.info(f"ğŸ“ åŸæ–‡é•¿åº¦: {len(text)} å­—ç¬¦")
        
        # âœ… å¦‚æœæ–‡æœ¬è¾ƒçŸ­ï¼Œç›´æ¥ç¿»è¯‘
        word_count = len(text.split())
        if word_count < 150:
            app_logger.info("ğŸ“„ æ–‡æœ¬è¾ƒçŸ­ï¼Œç›´æ¥ç¿»è¯‘")
            return translate_chunk(text, model, tokenizer, src_lang, tgt_lang)
        
        # âœ… é•¿æ–‡æœ¬åˆ†æ®µç¿»è¯‘
        app_logger.info("ğŸ“š æ–‡æœ¬è¾ƒé•¿ï¼Œåˆ†æ®µç¿»è¯‘")
        chunks = split_text_into_chunks(text, max_length=400)
        app_logger.info(f"ğŸ“¦ åˆ†ä¸º {len(chunks)} æ®µ")
        
        translated_chunks = []
        for i, chunk in enumerate(chunks, 1):
            app_logger.info(f"ğŸ”„ ç¿»è¯‘ç¬¬ {i}/{len(chunks)} æ®µ...")
            translated = translate_chunk(
                chunk, 
                model, 
                tokenizer, 
                src_lang, 
                tgt_lang
            )
            translated_chunks.append(translated)
        
        # åˆå¹¶ç¿»è¯‘ç»“æœ
        final_translation = ' '.join(translated_chunks)
        app_logger.info(f"âœ… ç¿»è¯‘å®Œæˆï¼Œè¯‘æ–‡é•¿åº¦: {len(final_translation)} å­—ç¬¦")
        
        return final_translation
    
    except Exception as e:
        app_logger.error(f"âŒ æ–‡æœ¬ç¿»è¯‘å¤±è´¥: {e}")
        raise


def batch_translate_texts(texts, src_lang='en', tgt_lang='zh'):
    """
    æ‰¹é‡ç¿»è¯‘å¤šä¸ªæ–‡æœ¬
    
    Args:
        texts: æ–‡æœ¬åˆ—è¡¨
        src_lang: æºè¯­è¨€ä»£ç 
        tgt_lang: ç›®æ ‡è¯­è¨€ä»£ç 
    
    Returns:
        list: ç¿»è¯‘åçš„æ–‡æœ¬åˆ—è¡¨
    """
    try:
        results = []
        
        for text in texts:
            if text.strip():
                translated = translate_text_with_nllb(text, src_lang, tgt_lang)
                results.append(translated)
            else:
                results.append('')
        
        return results
    
    except Exception as e:
        app_logger.error(f"âŒ æ‰¹é‡ç¿»è¯‘å¤±è´¥: {e}")
        raise