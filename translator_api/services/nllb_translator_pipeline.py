"""
NLLB (No Language Left Behind) ç¿»è¯‘æœåŠ¡
ä½¿ç”¨Metaçš„NLLBæ¨¡å‹è¿›è¡Œé«˜è´¨é‡å¤šè¯­è¨€ç¿»è¯‘
"""

import os
import logging
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# ã€æ–°å¢ã€‘å¯¼å…¥é…ç½®
try:
    from config import (
        NLLB_MODEL_NAME,
        NLLB_BATCH_SIZE,
        NLLB_MAX_LENGTH,
        NLLB_NUM_BEAMS,
        NLLB_USE_FP16,
        USE_GPU,
        GPU_DEVICE_ID,
        GPU_MEMORY_FRACTION,
        PYTORCH_CUDA_ALLOC_CONF
    )
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥,ä½¿ç”¨é»˜è®¤å€¼
    NLLB_MODEL_NAME = "facebook/nllb-200-distilled-600M"
    NLLB_BATCH_SIZE = 8
    NLLB_MAX_LENGTH = 200
    NLLB_NUM_BEAMS = 4
    NLLB_USE_FP16 = True
    USE_GPU = True
    GPU_DEVICE_ID = 0
    GPU_MEMORY_FRACTION = 0.7
    PYTORCH_CUDA_ALLOC_CONF = "expandable_segments:True"

logger = logging.getLogger(__name__)

class NLLBTranslator:
    """NLLBç¿»è¯‘å™¨"""
    
    def __init__(self, model_name=None):  # ã€ä¿®æ”¹ã€‘æ”¹ä¸ºå¯é€‰å‚æ•°
        """
        åˆå§‹åŒ–NLLBç¿»è¯‘å™¨
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™ä»é…ç½®æ–‡ä»¶è¯»å–
        """
        # ğŸ”¥ æ–°å¢: åˆå§‹åŒ– Ollama æœåŠ¡
        try:
            from services.ollama_service import ollama_service
            self.ollama_service = ollama_service
            logger.info("âœ“ Ollama AI æ€»ç»“æœåŠ¡å·²åŠ è½½")
        except Exception as e:
            logger.warning(f"âš ï¸ Ollama AI æ€»ç»“æœåŠ¡åŠ è½½å¤±è´¥: {e}")
            self.ollama_service = None
    
        # ã€ä¿®æ”¹ã€‘ä¼˜å…ˆä½¿ç”¨é…ç½®æ–‡ä»¶
        if model_name is None:
            model_name = NLLB_MODEL_NAME
        
        self.model_name = model_name
        
        # ã€ä¿®æ”¹ã€‘è®¾ç½®PyTorch CUDAå†…å­˜åˆ†é…å™¨
        if PYTORCH_CUDA_ALLOC_CONF:
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = PYTORCH_CUDA_ALLOC_CONF
            logger.info(f"âœ“ è®¾ç½® PYTORCH_CUDA_ALLOC_CONF: {PYTORCH_CUDA_ALLOC_CONF}")
        
        # ã€ä¿®æ”¹ã€‘æ ¹æ®é…ç½®é€‰æ‹©è®¾å¤‡
        if USE_GPU and torch.cuda.is_available():
            self.device = f"cuda:{GPU_DEVICE_ID}"
            logger.info(f"âœ“ ä½¿ç”¨ GPU: {torch.cuda.get_device_name(GPU_DEVICE_ID)}")
            logger.info(f"âœ“ GPU æ˜¾å­˜é™åˆ¶: {GPU_MEMORY_FRACTION * 100}%")
        else:
            self.device = "cpu"
            logger.info("âœ“ ä½¿ç”¨ CPU")
        
        self.tokenizer = None
        self.model = None
        
        # ã€æ–°å¢ã€‘å­˜å‚¨é…ç½®å‚æ•°
        self.batch_size = NLLB_BATCH_SIZE
        self.max_length = NLLB_MAX_LENGTH
        self.num_beams = NLLB_NUM_BEAMS
        self.use_fp16 = NLLB_USE_FP16
        
        # è¯­è¨€ä»£ç æ˜ å°„ (NLLBä½¿ç”¨ç‰¹æ®Šçš„è¯­è¨€ä»£ç )
        self.lang_map = {
            'zh': 'zho_Hans',
            'en': 'eng_Latn',
            'zh_cn': 'zho_Hans',
            'zh_tw': 'zho_Hant',
            'chinese': 'zho_Hans',
            'english': 'eng_Latn',
            'de': 'deu_Latn',
            'german': 'deu_Latn',
            'deutsch': 'deu_Latn',
            'fr': 'fra_Latn',
            'french': 'fra_Latn',
            'es': 'spa_Latn', 
            'spanish': 'spa_Latn',
            'ja': 'jpn_Jpan',
            'japanese': 'jpn_Jpan',
            'ko': 'kor_Hang',
            'korean': 'kor_Hang',
            'ru': 'rus_Cyrl',
            'russian': 'rus_Cyrl',
        }
        
        logger.info(f"âœ“ åˆå§‹åŒ– NLLB ç¿»è¯‘å™¨")
        logger.info(f"  æ¨¡å‹: {self.model_name}")
        logger.info(f"  è®¾å¤‡: {self.device}")
        logger.info(f"  æ‰¹æ¬¡å¤§å°: {self.batch_size}")
        logger.info(f"  æœ€å¤§é•¿åº¦: {self.max_length}")
        logger.info(f"  Beamæœç´¢: {self.num_beams}")
        logger.info(f"  FP16: {self.use_fp16}")
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        if self.model is not None:
            return
        
        logger.info(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {self.model_name}...")
        
        try:
            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            
            # ã€æ–°å¢ã€‘é¢„å…ˆè·å–lang_code_to_idçš„è®¿é—®æ–¹æ³•
            if hasattr(self.tokenizer, 'lang_code_to_id'):
                self._get_lang_token_id = lambda code: self.tokenizer.lang_code_to_id[code]
                logger.info("âœ“ ä½¿ç”¨ lang_code_to_id æ–¹æ³•")
            else:
                self._get_lang_token_id = lambda code: self.tokenizer.convert_tokens_to_ids(code)
                logger.info("âœ“ ä½¿ç”¨ convert_tokens_to_ids æ–¹æ³•")
            
            # æ ¹æ®é…ç½®åŠ è½½æ¨¡å‹
            if self.use_fp16 and self.device.startswith("cuda"):
                self.model = AutoModelForSeq2SeqLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.float16
                )
                logger.info("âœ“ ä½¿ç”¨ FP16 ç²¾åº¦")
            else:
                self.model = AutoModelForSeq2SeqLM.from_pretrained(
                    self.model_name
                )
                logger.info("âœ“ ä½¿ç”¨ FP32 ç²¾åº¦")
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            self.model = self.model.to(self.device)
            self.model.eval()
            
            logger.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {self.model_name}")
            
            # æ˜¾ç¤ºæ˜¾å­˜ä½¿ç”¨æƒ…å†µ
            if self.device.startswith("cuda"):
                allocated = torch.cuda.memory_allocated(GPU_DEVICE_ID) / 1024**3
                reserved = torch.cuda.memory_reserved(GPU_DEVICE_ID) / 1024**3
                logger.info(f"ğŸ“Š GPU æ˜¾å­˜: å·²åˆ†é… {allocated:.2f}GB, å·²é¢„ç•™ {reserved:.2f}GB")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def get_lang_code(self, lang):
        """è·å–NLLBè¯­è¨€ä»£ç """
        lang_lower = lang.lower()
        return self.lang_map.get(lang_lower, lang)
    
    def translate(self, text, src_lang='zh', tgt_lang='en'):
        """ç¿»è¯‘å•ä¸ªæ–‡æœ¬"""
        if not text or not text.strip():
            return ""
        
        self.load_model()
        
        src_code = self.get_lang_code(src_lang)
        tgt_code = self.get_lang_code(tgt_lang)
        
        # è®¾ç½®æºè¯­è¨€
        self.tokenizer.src_lang = src_code
        
        # ç¼–ç 
        inputs = self.tokenizer(
            text, 
            return_tensors="pt", 
            max_length=self.max_length,
            truncation=True
        ).to(self.device)
        
        # ç¿»è¯‘
        with torch.no_grad():
            translated_tokens = self.model.generate(
                **inputs,
                forced_bos_token_id=self._get_lang_token_id(tgt_code),  # ã€ä¿®å¤ã€‘ä½¿ç”¨ç»Ÿä¸€æ–¹æ³•
                max_length=self.max_length,
                num_beams=self.num_beams
            )
        
        # è§£ç 
        result = self.tokenizer.batch_decode(
            translated_tokens, 
            skip_special_tokens=True
        )[0]
        
        return result
    
    def translate_batch(self, texts, src_lang='zh', tgt_lang='en', batch_size=None):
        """æ‰¹é‡ç¿»è¯‘"""
        if not texts:
            return []
        
        self.load_model()
        
        if batch_size is None:
            batch_size = self.batch_size
        
        src_code = self.get_lang_code(src_lang)
        tgt_code = self.get_lang_code(tgt_lang)
        
        # è®¾ç½®æºè¯­è¨€
        self.tokenizer.src_lang = src_code
        
        results = []
        total_batches = (len(texts) + batch_size - 1) // batch_size
        
        logger.info(f"ğŸ“Š æ‰¹é‡ç¿»è¯‘: {len(texts)} ä¸ªæ–‡æœ¬, åˆ† {total_batches} æ‰¹")
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            
            # ç¼–ç 
            inputs = self.tokenizer(
                batch, 
                return_tensors="pt", 
                padding=True,
                max_length=self.max_length,
                truncation=True
            ).to(self.device)
            
            # ç¿»è¯‘
            with torch.no_grad():
                translated_tokens = self.model.generate(
                    **inputs,
                    forced_bos_token_id=self._get_lang_token_id(tgt_code),  # ã€ä¿®å¤ã€‘ä½¿ç”¨ç»Ÿä¸€æ–¹æ³•
                    max_length=self.max_length,
                    num_beams=self.num_beams
                )
            
            # è§£ç 
            batch_results = self.tokenizer.batch_decode(
                translated_tokens, 
                skip_special_tokens=True
            )
            
            results.extend(batch_results)
            
            # ã€æ–°å¢ã€‘æ˜¾ç¤ºè¿›åº¦
            if (i // batch_size + 1) % 10 == 0 or (i + batch_size) >= len(texts):
                logger.info(f"  è¿›åº¦: {len(results)}/{len(texts)}")
        
        return results
    
    def auto_translate(self, text):
        """è‡ªåŠ¨æ£€æµ‹è¯­è¨€å¹¶ç¿»è¯‘ï¼ˆä¸­->è‹± æˆ– è‹±->ä¸­ï¼‰"""
        # ç®€å•æ£€æµ‹ï¼šåŒ…å«ä¸­æ–‡å­—ç¬¦åˆ™è®¤ä¸ºæ˜¯ä¸­æ–‡
        has_chinese = any('\u4e00' <= char <= '\u9fff' for char in text)
        
        if has_chinese:
            return self.translate(text, 'zh', 'en')
        else:
            return self.translate(text, 'en', 'zh')

    # ğŸ”¥ æ–°å¢: å¸¦AIæ€»ç»“çš„ç¿»è¯‘æ–¹æ³•
    def translate_with_summary(
        self,
        texts,
        src_lang='zh',
        tgt_lang='en',
        batch_size=None,
        enable_summary=False
    ):
        """
        æ‰¹é‡ç¿»è¯‘å¹¶å¯é€‰ç”ŸæˆAIæ€»ç»“
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            src_lang: æºè¯­è¨€
            tgt_lang: ç›®æ ‡è¯­è¨€
            batch_size: æ‰¹æ¬¡å¤§å°
            enable_summary: æ˜¯å¦å¯ç”¨AIæ€»ç»“
        
        Returns:
            {
                'translations': List[str],  # ç¿»è¯‘ç»“æœ
                'summary': Dict or None     # AIæ€»ç»“ç»“æœ
            }
        """
        # 1. æ‰§è¡Œç¿»è¯‘
        translations = self.translate_batch(texts, src_lang, tgt_lang, batch_size)
        
        result = {
            'translations': translations,
            'summary': None
        }
        
        # 2. å¦‚æœå¯ç”¨æ€»ç»“,ç”ŸæˆAIæ€»ç»“
        if enable_summary and self.ollama_service:
            try:
                # åˆå¹¶æ‰€æœ‰ç¿»è¯‘ç»“æœ
                combined_text = '\n'.join(translations)
                
                if combined_text.strip():
                    logger.info(f"ğŸ§  å¼€å§‹ç”ŸæˆAIæ€»ç»“...")
                    summary_result = self.ollama_service.generate_summary(
                        text=combined_text,
                        target_language=tgt_lang
                    )
                    
                    result['summary'] = summary_result
                    
                    if summary_result.get('success'):
                        logger.info(f"âœ“ AIæ€»ç»“ç”ŸæˆæˆåŠŸ")
                    else:
                        logger.warning(f"âš ï¸ AIæ€»ç»“ç”Ÿæˆå¤±è´¥: {summary_result.get('error')}")
                        
            except Exception as e:
                logger.error(f"âŒ AIæ€»ç»“å¼‚å¸¸: {e}")
                result['summary'] = {
                    'success': False,
                    'summary': None,
                    'error': 'ç”Ÿæˆæ€»ç»“æ—¶å‘ç”Ÿé”™è¯¯ ğŸ”§'
                }
        elif enable_summary and not self.ollama_service:
            logger.warning("âš ï¸ AIæ€»ç»“æœåŠ¡æœªåŠ è½½")
            result['summary'] = {
                'success': False,
                'summary': None,
                'error': 'AIæ€»ç»“æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ ğŸ˜Š'
            }
        
        return result

# å•ä¾‹æ¨¡å¼
_translator_instance = None

def get_translator(model_name=None):  # ã€ä¿®æ”¹ã€‘æ·»åŠ å¯é€‰å‚æ•°
    """è·å–ç¿»è¯‘å™¨å•ä¾‹"""
    global _translator_instance
    if _translator_instance is None:
        _translator_instance = NLLBTranslator(model_name)
    return _translator_instance


def translate_text(text, src_lang='zh', tgt_lang='en'):
    """ä¾¿æ·å‡½æ•°ï¼šç¿»è¯‘å•ä¸ªæ–‡æœ¬"""
    translator = get_translator()
    return translator.translate(text, src_lang, tgt_lang)


def translate_texts(texts, src_lang='zh', tgt_lang='en'):
    """ä¾¿æ·å‡½æ•°ï¼šæ‰¹é‡ç¿»è¯‘"""
    translator = get_translator()
    return translator.translate_batch(texts, src_lang, tgt_lang)


# æµ‹è¯•å‡½æ•°
def test_translator():
    """æµ‹è¯•ç¿»è¯‘å™¨"""
    print("=" * 70)
    print("NLLB ç¿»è¯‘å™¨æµ‹è¯•")
    print("=" * 70)
    
    translator = get_translator()
    
    # æµ‹è¯•1: ä¸­æ–‡->è‹±æ–‡
    print("\nã€æµ‹è¯• 1ã€‘ä¸­æ–‡ -> è‹±æ–‡")
    text_zh = "ä½ å¥½ï¼Œä¸–ç•Œï¼è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•ã€‚"
    result = translator.translate(text_zh, 'zh', 'en')
    print(f"åŸæ–‡: {text_zh}")
    print(f"è¯‘æ–‡: {result}")
    
    # æµ‹è¯•2: è‹±æ–‡->ä¸­æ–‡
    print("\nã€æµ‹è¯• 2ã€‘è‹±æ–‡ -> ä¸­æ–‡")
    text_en = "Hello, world! This is a test."
    result = translator.translate(text_en, 'en', 'zh')
    print(f"åŸæ–‡: {text_en}")
    print(f"è¯‘æ–‡: {result}")
    
    # æµ‹è¯•3: æ‰¹é‡ç¿»è¯‘
    print("\nã€æµ‹è¯• 3ã€‘æ‰¹é‡ç¿»è¯‘")
    texts = ["ä½ å¥½", "ä¸–ç•Œ", "æµ‹è¯•"]
    results = translator.translate_batch(texts, 'zh', 'en')
    for orig, trans in zip(texts, results):
        print(f"  {orig} -> {trans}")
    
    print("\n" + "=" * 70)
    print("æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    test_translator()