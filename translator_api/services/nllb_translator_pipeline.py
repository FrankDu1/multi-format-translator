"""
NLLB (No Language Left Behind) ç¿»è¯‘æœåŠ¡
ä½¿ç”¨Metaçš„NLLBæ¨¡å‹è¿›è¡Œé«˜è´¨é‡å¤šè¯­è¨€ç¿»è¯‘
"""

import os
import logging
import torch
import re
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from dotenv import load_dotenv
load_dotenv()
from services.ali_translate_client import AliTranslateClient
from logger_config import app_logger, api_logger, log_exception
import concurrent.futures

USE_CLOUD_TRANSLATE = os.getenv('USE_CLOUD_TRANSLATE', 'false').lower() == 'true'
app_logger.info(f"[å¯åŠ¨] USE_CLOUD_TRANSLATE ç¯å¢ƒå˜é‡: {os.getenv('USE_CLOUD_TRANSLATE')}")
app_logger.info(f"[å¯åŠ¨] USE_CLOUD_TRANSLATE è§£æç»“æœ: {USE_CLOUD_TRANSLATE}")

# ã€æ–°å¢ã€‘å¯¼å…¥é…ç½®
try:
    from config import (
        NLLB_MODEL_NAME,
        NLLB_BATCH_SIZE,
        NLLB_MAX_LENGTH,
        NLLB_NUM_BEAMS,
        NLLB_USE_FP16,
        USE_GPU,
        GPU_DEVICE_ID,
        GPU_MEMORY_FRACTION,
        PYTORCH_CUDA_ALLOC_CONF
    )
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥,ä½¿ç”¨é»˜è®¤å€¼
    NLLB_MODEL_NAME = "facebook/nllb-200-distilled-600M"
    NLLB_BATCH_SIZE = 8
    NLLB_MAX_LENGTH = 200
    NLLB_NUM_BEAMS = 4
    NLLB_USE_FP16 = True
    USE_GPU = True
    GPU_DEVICE_ID = 0
    GPU_MEMORY_FRACTION = 0.7
    PYTORCH_CUDA_ALLOC_CONF = "expandable_segments:True"

logger = logging.getLogger(__name__)

class NLLBTranslator:
    """NLLBç¿»è¯‘å™¨"""
    
    def __init__(self, model_name=None):  # ã€ä¿®æ”¹ã€‘æ”¹ä¸ºå¯é€‰å‚æ•°
        """
        åˆå§‹åŒ–NLLBç¿»è¯‘å™¨
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™ä»é…ç½®æ–‡ä»¶è¯»å–
        """
        # ğŸ”¥ åŠ¨æ€å¯¼å…¥ AI æœåŠ¡
        try:
            from config import AI_PROVIDER
            if AI_PROVIDER == 'qwen':
                from services.qwen_service import qwen_service
                self.ollama_service = qwen_service
            else:
                from services.ollama_service import ollama_service
                self.ollama_service = ollama_service
            logger.info(f"âœ“ AIæ€»ç»“æœåŠ¡å·²åŠ è½½ (æä¾›å•†: {AI_PROVIDER})")
        except Exception as e:
            logger.warning(f"âš ï¸ AIæ€»ç»“æœåŠ¡åŠ è½½å¤±è´¥: {e}")
            self.ollama_service = None
    
        # ã€ä¿®æ”¹ã€‘ä¼˜å…ˆä½¿ç”¨é…ç½®æ–‡ä»¶
        if model_name is None:
            model_name = NLLB_MODEL_NAME
        
        self.model_name = model_name
        
        # ã€ä¿®æ”¹ã€‘è®¾ç½®PyTorch CUDAå†…å­˜åˆ†é…å™¨
        if PYTORCH_CUDA_ALLOC_CONF:
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = PYTORCH_CUDA_ALLOC_CONF
            logger.info(f"âœ“ è®¾ç½® PYTORCH_CUDA_ALLOC_CONF: {PYTORCH_CUDA_ALLOC_CONF}")
        
        # ã€ä¿®æ”¹ã€‘æ ¹æ®é…ç½®é€‰æ‹©è®¾å¤‡
        if USE_GPU and torch.cuda.is_available():
            self.device = f"cuda:{GPU_DEVICE_ID}"
            logger.info(f"âœ“ ä½¿ç”¨ GPU: {torch.cuda.get_device_name(GPU_DEVICE_ID)}")
            logger.info(f"âœ“ GPU æ˜¾å­˜é™åˆ¶: {GPU_MEMORY_FRACTION * 100}%")
        else:
            self.device = "cpu"
            logger.info("âœ“ ä½¿ç”¨ CPU")
        
        self.tokenizer = None
        self.model = None
        
        # ã€æ–°å¢ã€‘å­˜å‚¨é…ç½®å‚æ•°
        self.batch_size = NLLB_BATCH_SIZE
        self.max_length = NLLB_MAX_LENGTH
        self.num_beams = NLLB_NUM_BEAMS
        self.use_fp16 = NLLB_USE_FP16
        
        # è¯­è¨€ä»£ç æ˜ å°„ (NLLBä½¿ç”¨ç‰¹æ®Šçš„è¯­è¨€ä»£ç )
        self.lang_map = {
            'zh': 'zho_Hans',
            'en': 'eng_Latn',
            'zh_cn': 'zho_Hans',
            'zh_tw': 'zho_Hant',
            'chinese': 'zho_Hans',
            'english': 'eng_Latn',
            'de': 'deu_Latn',
            'german': 'deu_Latn',
            'deutsch': 'deu_Latn',
            'fr': 'fra_Latn',
            'french': 'fra_Latn',
            'es': 'spa_Latn', 
            'spanish': 'spa_Latn',
            'ja': 'jpn_Jpan',
            'japanese': 'jpn_Jpan',
            'ko': 'kor_Hang',
            'korean': 'kor_Hang',
            'ru': 'rus_Cyrl',
            'russian': 'rus_Cyrl',
        }
        
        logger.info(f"âœ“ åˆå§‹åŒ– NLLB ç¿»è¯‘å™¨")
        logger.info(f"  æ¨¡å‹: {self.model_name}")
        logger.info(f"  è®¾å¤‡: {self.device}")
        logger.info(f"  æ‰¹æ¬¡å¤§å°: {self.batch_size}")
        logger.info(f"  æœ€å¤§é•¿åº¦: {self.max_length}")
        logger.info(f"  Beamæœç´¢: {self.num_beams}")
        logger.info(f"  FP16: {self.use_fp16}")
    
    def _translate_batch_cloud_smart(self, texts, src_lang='zh', tgt_lang='en'):
        """
        äº‘ç«¯ç¿»è¯‘ - æ™ºèƒ½åˆ†ç»„ç­–ç•¥
        
        ç­–ç•¥ï¼š
        1. çŸ­æ–‡æœ¬ï¼ˆ<30å­—ç¬¦ï¼‰ï¼šå•ç‹¬ç¿»è¯‘ï¼ˆè¡¨æ ¼å•å…ƒæ ¼ã€åˆ—è¡¨æ ‡è®°ï¼‰
        2. é•¿æ–‡æœ¬ï¼ˆ>=30å­—ç¬¦ï¼‰ï¼šæ™ºèƒ½åˆå¹¶ç¿»è¯‘ï¼ˆæœ€å¤š5ä¸ªä¸€ç»„ï¼Œæ€»é•¿åº¦<900å­—ç¬¦ï¼‰
        """
        from concurrent.futures import ThreadPoolExecutor
        import time
        
        client = AliTranslateClient()
        
        logger.info(f"ğŸ“Š [äº‘ç«¯æ™ºèƒ½] å¼€å§‹ç¿»è¯‘ {len(texts)} ä¸ªæ–‡æœ¬ç‰‡æ®µ...")
        
        # æ­¥éª¤1ï¼šåˆ†ææ–‡æœ¬ï¼Œåˆ†ä¸ºçŸ­æ–‡æœ¬å’Œé•¿æ–‡æœ¬
        short_texts = []  # [(index, text)]
        long_texts = []   # [(index, text)]
        
        for idx, text in enumerate(texts):
            if not text or not str(text).strip():
                continue
            
            cleaned = str(text).strip()
            text_len = len(cleaned)
            
            if text_len < 30:
                short_texts.append((idx, cleaned))
            else:
                long_texts.append((idx, cleaned))
        
        logger.info(f"  - çŸ­æ–‡æœ¬: {len(short_texts)} ä¸ª (å•ç‹¬ç¿»è¯‘)")
        logger.info(f"  - é•¿æ–‡æœ¬: {len(long_texts)} ä¸ª (æ™ºèƒ½åˆå¹¶)")
        
        # åˆå§‹åŒ–ç»“æœæ•°ç»„
        results = [''] * len(texts)
        
        # æ­¥éª¤2ï¼šå¹¶å‘ç¿»è¯‘çŸ­æ–‡æœ¬
        def translate_single(item):
            idx, text = item
            max_retries = 3
            
            for attempt in range(max_retries):
                try:
                    result = client.translate(text, source_lang=src_lang, target_lang=tgt_lang)
                    if result.get('success'):
                        return idx, result.get('translated_text', text)
                except Exception as e:
                    if attempt < max_retries - 1:
                        time.sleep(0.5)
                    else:
                        logger.warning(f"âš ï¸ ç¿»è¯‘å¤±è´¥: {text[:20]}...")
            
            return idx, text  # å¤±è´¥è¿”å›åŸæ–‡
        
        if short_texts:
            logger.info(f"ğŸ”„ å¹¶å‘ç¿»è¯‘ {len(short_texts)} ä¸ªçŸ­æ–‡æœ¬...")
            with ThreadPoolExecutor(max_workers=10) as executor:
                for idx, translated in executor.map(translate_single, short_texts):
                    results[idx] = translated
            logger.info(f"âœ… çŸ­æ–‡æœ¬ç¿»è¯‘å®Œæˆ")
        
        # æ­¥éª¤3ï¼šæ™ºèƒ½åˆå¹¶ç¿»è¯‘é•¿æ–‡æœ¬
        if long_texts:
            logger.info(f"ğŸ”„ æ™ºèƒ½åˆå¹¶ç¿»è¯‘ {len(long_texts)} ä¸ªé•¿æ–‡æœ¬...")
            
            # åˆ†ç»„ï¼šæ¯ç»„æœ€å¤š5ä¸ªæ–‡æœ¬ï¼Œæ€»é•¿åº¦<900å­—ç¬¦
            groups = []
            current_group = []
            current_length = 0
            max_group_size = 5
            max_group_length = 900
            
            for idx, text in long_texts:
                text_len = len(text)
                
                # åˆ¤æ–­æ˜¯å¦éœ€è¦æ–°å»ºç»„
                if (current_group and 
                    (len(current_group) >= max_group_size or 
                     current_length + text_len > max_group_length)):
                    groups.append(current_group)
                    current_group = []
                    current_length = 0
                
                current_group.append((idx, text))
                current_length += text_len
            
            if current_group:
                groups.append(current_group)
            
            logger.info(f"  åˆ†ä¸º {len(groups)} ä¸ªåˆå¹¶ç»„")
            
            # ç¿»è¯‘æ¯ä¸ªç»„
            for group_idx, group in enumerate(groups):
                if (group_idx + 1) % 5 == 0 or group_idx == len(groups) - 1:
                    logger.info(f"  è¿›åº¦: {group_idx + 1}/{len(groups)}")
                
                # ä½¿ç”¨æ¢è¡Œä½œä¸ºåˆ†éš”ç¬¦ï¼ˆæ›´è‡ªç„¶ï¼‰
                separator = "\n\n"
                combined_text = separator.join([text for _, text in group])
                
                # ç¿»è¯‘
                max_retries = 3
                translated = None
                
                for attempt in range(max_retries):
                    try:
                        result = client.translate(
                            combined_text, 
                            source_lang=src_lang, 
                            target_lang=tgt_lang
                        )
                        
                        if result.get('success'):
                            translated = result.get('translated_text', '').strip()
                            break
                    except Exception as e:
                        logger.warning(f"âš ï¸ è¯·æ±‚å¼‚å¸¸ (å°è¯•{attempt+1}/{max_retries}): {e}")
                    
                    if attempt < max_retries - 1:
                        time.sleep(1 * (attempt + 1))
                
                if not translated:
                    # ç¿»è¯‘å¤±è´¥ï¼Œä½¿ç”¨åŸæ–‡
                    logger.error(f"âŒ ç»„{group_idx+1}ç¿»è¯‘å¤±è´¥ï¼Œä½¿ç”¨åŸæ–‡")
                    for idx, text in group:
                        results[idx] = text
                    continue
                
                # æ™ºèƒ½åˆ†å‰²ç¿»è¯‘ç»“æœ
                translated_parts = translated.split('\n\n')
                
                # å¦‚æœåˆ†å‰²æ•°é‡ä¸åŒ¹é…
                if len(translated_parts) != len(group):
                    # å°è¯•æŒ‰å•ä¸ªæ¢è¡Œç¬¦åˆ†å‰²
                    translated_parts = translated.split('\n')
                    translated_parts = [p.strip() for p in translated_parts if p.strip()]
                
                # å¦‚æœè¿˜æ˜¯ä¸åŒ¹é…ï¼ŒæŒ‰æ¯”ä¾‹åˆ†å‰²
                if len(translated_parts) != len(group):
                    translated_parts = self._split_by_ratio(translated, len(group))
                
                # åˆ†é…ç»“æœ
                for i, (idx, original_text) in enumerate(group):
                    if i < len(translated_parts):
                        results[idx] = translated_parts[i].strip() or original_text
                    else:
                        results[idx] = original_text
            
            logger.info(f"âœ… é•¿æ–‡æœ¬ç¿»è¯‘å®Œæˆ")
        
        logger.info(f"âœ… [äº‘ç«¯æ™ºèƒ½] ç¿»è¯‘å®Œæˆ")
        return results
    
    def _translate_batch_cloud_individual(self, texts, src_lang='zh', tgt_lang='en'):
        """
        äº‘ç«¯ç¿»è¯‘ - é€æ¡ç¿»è¯‘æ¨¡å¼ï¼ˆç”¨äºéœ€è¦ç²¾ç¡®ä½ç½®å¯¹åº”çš„åœºæ™¯ï¼Œå¦‚PDFï¼‰
        
        ç¡®ä¿æ¯ä¸ªè¾“å…¥æ–‡æœ¬éƒ½æœ‰ä¸€ä¸ªå¯¹åº”çš„è¾“å‡ºæ–‡æœ¬ï¼Œä¸ä¼šå› ä¸ºåˆ†ç»„åˆå¹¶å¯¼è‡´æ•°é‡ä¸åŒ¹é…
        """
        from concurrent.futures import ThreadPoolExecutor
        import time
        
        client = AliTranslateClient()
        
        logger.info(f"ğŸ“Š [äº‘ç«¯é€æ¡] å¼€å§‹ç¿»è¯‘ {len(texts)} ä¸ªæ–‡æœ¬ç‰‡æ®µ...")
        
        results = [''] * len(texts)
        
        def translate_single(item):
            idx, text = item
            
            if not text or not str(text).strip():
                return idx, ''
            
            cleaned = str(text).strip()
            max_retries = 3
            
            for attempt in range(max_retries):
                try:
                    result = client.translate(cleaned, source_lang=src_lang, target_lang=tgt_lang)
                    if result.get('success'):
                        return idx, result.get('translated_text', cleaned)
                except Exception as e:
                    if attempt < max_retries - 1:
                        time.sleep(0.5)
                    else:
                        logger.warning(f"âš ï¸ ç¿»è¯‘å¤±è´¥: {cleaned[:20]}...")
            
            return idx, cleaned  # å¤±è´¥è¿”å›åŸæ–‡
        
        # å¹¶å‘ç¿»è¯‘æ‰€æœ‰æ–‡æœ¬
        logger.info(f"ğŸ”„ å¹¶å‘ç¿»è¯‘ä¸­ï¼ˆ10çº¿ç¨‹ï¼‰...")
        with ThreadPoolExecutor(max_workers=10) as executor:
            for idx, translated in executor.map(translate_single, enumerate(texts)):
                results[idx] = translated
                
                # æ˜¾ç¤ºè¿›åº¦
                if (idx + 1) % 20 == 0 or (idx + 1) == len(texts):
                    logger.info(f"  è¿›åº¦: {idx + 1}/{len(texts)}")
        
        logger.info(f"âœ… [äº‘ç«¯é€æ¡] ç¿»è¯‘å®Œæˆ")
        return results
    
    def _split_by_ratio(self, text, num_parts):
        """æŒ‰æ¯”ä¾‹åˆ†å‰²æ–‡æœ¬ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        if num_parts <= 1:
            return [text]
        
        # å°è¯•æŒ‰æ ‡ç‚¹ç¬¦å·åˆ†å‰²
        sentences = re.split(r'[ã€‚.!?ï¼ï¼Ÿ\n]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if len(sentences) >= num_parts:
            # å¹³å‡åˆ†é…å¥å­
            result = []
            sentences_per_part = len(sentences) // num_parts
            
            for i in range(num_parts):
                start = i * sentences_per_part
                end = start + sentences_per_part if i < num_parts - 1 else len(sentences)
                part = ' '.join(sentences[start:end])
                result.append(part)
            
            return result
        
        # å¦‚æœå¥å­æ•°ä¸å¤Ÿï¼ŒæŒ‰é•¿åº¦åˆ†å‰²
        part_length = len(text) // num_parts
        parts = []
        
        for i in range(num_parts):
            start = i * part_length
            end = start + part_length if i < num_parts - 1 else len(text)
            parts.append(text[start:end].strip())
        
        return parts

    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        if self.model is not None:
            return
        
        logger.info(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {self.model_name}...")
        
        try:
            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            
            # ã€æ–°å¢ã€‘é¢„å…ˆè·å–lang_code_to_idçš„è®¿é—®æ–¹æ³•
            if hasattr(self.tokenizer, 'lang_code_to_id'):
                self._get_lang_token_id = lambda code: self.tokenizer.lang_code_to_id[code]
                logger.info("âœ“ ä½¿ç”¨ lang_code_to_id æ–¹æ³•")
            else:
                self._get_lang_token_id = lambda code: self.tokenizer.convert_tokens_to_ids(code)
                logger.info("âœ“ ä½¿ç”¨ convert_tokens_to_ids æ–¹æ³•")
            
            # æ ¹æ®é…ç½®åŠ è½½æ¨¡å‹
            if self.use_fp16 and self.device.startswith("cuda"):
                self.model = AutoModelForSeq2SeqLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.float16
                )
                logger.info("âœ“ ä½¿ç”¨ FP16 ç²¾åº¦")
            else:
                self.model = AutoModelForSeq2SeqLM.from_pretrained(
                    self.model_name
                )
                logger.info("âœ“ ä½¿ç”¨ FP32 ç²¾åº¦")
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            self.model = self.model.to(self.device)
            self.model.eval()
            
            logger.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {self.model_name}")
            
            # æ˜¾ç¤ºæ˜¾å­˜ä½¿ç”¨æƒ…å†µ
            if self.device.startswith("cuda"):
                allocated = torch.cuda.memory_allocated(GPU_DEVICE_ID) / 1024**3
                reserved = torch.cuda.memory_reserved(GPU_DEVICE_ID) / 1024**3
                logger.info(f"ğŸ“Š GPU æ˜¾å­˜: å·²åˆ†é… {allocated:.2f}GB, å·²é¢„ç•™ {reserved:.2f}GB")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def get_lang_code(self, lang):
        """è·å–NLLBè¯­è¨€ä»£ç """
        lang_lower = lang.lower()
        return self.lang_map.get(lang_lower, lang)
    
    def translate(self, text, src_lang='zh', tgt_lang='en'):
        """ç¿»è¯‘å•ä¸ªæ–‡æœ¬"""
        if not text or not text.strip():
            return ""
        
        self.load_model()
        
        src_code = self.get_lang_code(src_lang)
        tgt_code = self.get_lang_code(tgt_lang)
        
        if USE_CLOUD_TRANSLATE:
            client = AliTranslateClient()
            result = client.translate(text, source_lang=src_lang, target_lang=tgt_lang)
            if result.get('success'):
                return result.get('translated_text', '')
            else:
                return text 

        # è®¾ç½®æºè¯­è¨€
        self.tokenizer.src_lang = src_code
        
        # ç¼–ç 
        inputs = self.tokenizer(
            text, 
            return_tensors="pt", 
            max_length=self.max_length,
            truncation=True
        ).to(self.device)
        
        # ç¿»è¯‘
        with torch.no_grad():
            translated_tokens = self.model.generate(
                **inputs,
                forced_bos_token_id=self._get_lang_token_id(tgt_code),  # ã€ä¿®å¤ã€‘ä½¿ç”¨ç»Ÿä¸€æ–¹æ³•
                max_length=self.max_length,
                num_beams=self.num_beams
            )
        
        # è§£ç 
        result = self.tokenizer.batch_decode(
            translated_tokens, 
            skip_special_tokens=True
        )[0]
        
        return result
    
    def translate_batch(self, texts, src_lang='zh', tgt_lang='en', batch_size=None, force_individual=False):
        """æ‰¹é‡ç¿»è¯‘ - ä¼˜åŒ–äº‘ç«¯ç¿»è¯‘ç‰ˆ
        
        Args:
            texts: è¦ç¿»è¯‘çš„æ–‡æœ¬åˆ—è¡¨
            src_lang: æºè¯­è¨€
            tgt_lang: ç›®æ ‡è¯­è¨€
            batch_size: æ‰¹æ¬¡å¤§å°
            force_individual: å¼ºåˆ¶é€æ¡ç¿»è¯‘ï¼ˆç”¨äºPDFç­‰éœ€è¦ç²¾ç¡®ä½ç½®å¯¹åº”çš„åœºæ™¯ï¼‰
        """
        if not texts:
            return []

        self.load_model()

        if batch_size is None:
            batch_size = self.batch_size

        src_code = self.get_lang_code(src_lang)
        tgt_code = self.get_lang_code(tgt_lang)

        # äº‘ç«¯ç¿»è¯‘
        if USE_CLOUD_TRANSLATE:
            # ğŸ”¥ å¦‚æœå¼ºåˆ¶é€æ¡ç¿»è¯‘ï¼Œä½¿ç”¨ç®€å•æ¨¡å¼
            if force_individual:
                return self._translate_batch_cloud_individual(texts, src_lang, tgt_lang)
            else:
                return self._translate_batch_cloud_smart(texts, src_lang, tgt_lang)

        # æœ¬åœ°ç¿»è¯‘ï¼Œè®¾ç½®æºè¯­è¨€
        self.tokenizer.src_lang = src_code
        
        results = []
        total_batches = (len(texts) + batch_size - 1) // batch_size
        
        logger.info(f"ğŸ“Š æ‰¹é‡ç¿»è¯‘: {len(texts)} ä¸ªæ–‡æœ¬, åˆ† {total_batches} æ‰¹")
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            
            # ç¼–ç 
            inputs = self.tokenizer(
                batch, 
                return_tensors="pt", 
                padding=True,
                max_length=self.max_length,
                truncation=True
            ).to(self.device)
            
            # ç¿»è¯‘
            with torch.no_grad():
                translated_tokens = self.model.generate(
                    **inputs,
                    forced_bos_token_id=self._get_lang_token_id(tgt_code),  # ã€ä¿®å¤ã€‘ä½¿ç”¨ç»Ÿä¸€æ–¹æ³•
                    max_length=self.max_length,
                    num_beams=self.num_beams
                )
            
            # è§£ç 
            batch_results = self.tokenizer.batch_decode(
                translated_tokens, 
                skip_special_tokens=True
            )
            
            results.extend(batch_results)

            # æ˜¾ç¤ºè¿›åº¦
            if (i // batch_size + 1) % 10 == 0 or (i + batch_size) >= len(texts):
                logger.info(f"  è¿›åº¦: {len(results)}/{len(texts)}")
        
        return results
    
    def auto_translate(self, text):
        """è‡ªåŠ¨æ£€æµ‹è¯­è¨€å¹¶ç¿»è¯‘ï¼ˆä¸­->è‹± æˆ– è‹±->ä¸­ï¼‰"""
        # ç®€å•æ£€æµ‹ï¼šåŒ…å«ä¸­æ–‡å­—ç¬¦åˆ™è®¤ä¸ºæ˜¯ä¸­æ–‡
        has_chinese = any('\u4e00' <= char <= '\u9fff' for char in text)
        
        if has_chinese:
            return self.translate(text, 'zh', 'en')
        else:
            return self.translate(text, 'en', 'zh')

    # ğŸ”¥ æ–°å¢: å¸¦AIæ€»ç»“çš„ç¿»è¯‘æ–¹æ³•
    def translate_with_summary(
        self,
        texts,
        src_lang='zh',
        tgt_lang='en',
        batch_size=None,
        enable_summary=False
    ):
        """
        æ‰¹é‡ç¿»è¯‘å¹¶å¯é€‰ç”ŸæˆAIæ€»ç»“
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            src_lang: æºè¯­è¨€
            tgt_lang: ç›®æ ‡è¯­è¨€
            batch_size: æ‰¹æ¬¡å¤§å°
            enable_summary: æ˜¯å¦å¯ç”¨AIæ€»ç»“
        
        Returns:
            {
                'translations': List[str],  # ç¿»è¯‘ç»“æœ
                'summary': Dict or None     # AIæ€»ç»“ç»“æœ
            }
        """
        # 1. æ‰§è¡Œç¿»è¯‘
        translations = self.translate_batch(texts, src_lang, tgt_lang, batch_size)
        
        result = {
            'translations': translations,
            'summary': None
        }
        
        # 2. å¦‚æœå¯ç”¨æ€»ç»“,ç”ŸæˆAIæ€»ç»“
        if enable_summary and self.ollama_service:
            try:
                # åˆå¹¶æ‰€æœ‰ç¿»è¯‘ç»“æœ
                combined_text = '\n'.join(translations)
                
                if combined_text.strip():
                    logger.info(f"ğŸ§  å¼€å§‹ç”ŸæˆAIæ€»ç»“...")
                    summary_result = self.ollama_service.generate_summary(
                        text=combined_text,
                        target_language=tgt_lang
                    )
                    
                    result['summary'] = summary_result
                    
                    if summary_result.get('success'):
                        logger.info(f"âœ“ AIæ€»ç»“ç”ŸæˆæˆåŠŸ")
                    else:
                        logger.warning(f"âš ï¸ AIæ€»ç»“ç”Ÿæˆå¤±è´¥: {summary_result.get('error')}")
                        
            except Exception as e:
                logger.error(f"âŒ AIæ€»ç»“å¼‚å¸¸: {e}")
                result['summary'] = {
                    'success': False,
                    'summary': None,
                    'error': 'ç”Ÿæˆæ€»ç»“æ—¶å‘ç”Ÿé”™è¯¯ ğŸ”§'
                }
        elif enable_summary and not self.ollama_service:
            logger.warning("âš ï¸ AIæ€»ç»“æœåŠ¡æœªåŠ è½½")
            result['summary'] = {
                'success': False,
                'summary': None,
                'error': 'AIæ€»ç»“æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ ğŸ˜Š'
            }
        
        return result

# å•ä¾‹æ¨¡å¼
_translator_instance = None

def get_translator(model_name=None):  # ã€ä¿®æ”¹ã€‘æ·»åŠ å¯é€‰å‚æ•°
    """è·å–ç¿»è¯‘å™¨å•ä¾‹"""
    global _translator_instance
    if _translator_instance is None:
        _translator_instance = NLLBTranslator(model_name)
    return _translator_instance


def translate_text(text, src_lang='zh', tgt_lang='en'):
    """ä¾¿æ·å‡½æ•°ï¼šç¿»è¯‘å•ä¸ªæ–‡æœ¬"""
    translator = get_translator()
    return translator.translate(text, src_lang, tgt_lang)


def translate_texts(texts, src_lang='zh', tgt_lang='en'):
    """ä¾¿æ·å‡½æ•°ï¼šæ‰¹é‡ç¿»è¯‘"""
    translator = get_translator()
    return translator.translate_batch(texts, src_lang, tgt_lang)


# æµ‹è¯•å‡½æ•°
def test_translator():
    """æµ‹è¯•ç¿»è¯‘å™¨"""
    print("=" * 70)
    print("NLLB ç¿»è¯‘å™¨æµ‹è¯•")
    print("=" * 70)
    
    translator = get_translator()
    
    # æµ‹è¯•1: ä¸­æ–‡->è‹±æ–‡
    print("\nã€æµ‹è¯• 1ã€‘ä¸­æ–‡ -> è‹±æ–‡")
    text_zh = "ä½ å¥½ï¼Œä¸–ç•Œï¼è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•ã€‚"
    result = translator.translate(text_zh, 'zh', 'en')
    print(f"åŸæ–‡: {text_zh}")
    print(f"è¯‘æ–‡: {result}")
    
    # æµ‹è¯•2: è‹±æ–‡->ä¸­æ–‡
    print("\nã€æµ‹è¯• 2ã€‘è‹±æ–‡ -> ä¸­æ–‡")
    text_en = "Hello, world! This is a test."
    result = translator.translate(text_en, 'en', 'zh')
    print(f"åŸæ–‡: {text_en}")
    print(f"è¯‘æ–‡: {result}")
    
    # æµ‹è¯•3: æ‰¹é‡ç¿»è¯‘
    print("\nã€æµ‹è¯• 3ã€‘æ‰¹é‡ç¿»è¯‘")
    texts = ["ä½ å¥½", "ä¸–ç•Œ", "æµ‹è¯•"]
    results = translator.translate_batch(texts, 'zh', 'en')
    for orig, trans in zip(texts, results):
        print(f"  {orig} -> {trans}")
    
    print("\n" + "=" * 70)
    print("æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    test_translator()